# (PART) Higher foundations {-}

# Bell's theorem {#bells-theorem}

> About **quantum correlations**, which are stronger than any correlations allowed by classical physics, and about the **CHSH inequality** (used to prove a variant of **Bell's theorem**) which demonstrates this fact.

Every now and then, it is nice to put down your lecture notes and go and see how things actually work in the real world.
What is particularly wonderful (and maybe surprising) about quantum theory is that it turns up in many places where we might not expect it to.
One such example is in the polarisation of light, where we stumble across an intriguing paradox.

The (much-simplified) one sentence introduction to light polarisation is this: light is made of [**transverse**](https://en.wikipedia.org/wiki/Transverse_wave) waves, and transverse waves have a "direction", which we call **polarisation**; a **polarising filter** only allows waves of a certain polarisation to pass through.
If we take two polarising filters, and place them on top of each other with their polarisations oriented at $90^\circ$ to one another, then basically no light will pass through, since the only light that can pass through the first filter is orthogonally polarised with respect to the second filter, and is thus blocked from passing through.
But then, if we take a third filter, and place it in between the other two, at an angle in the middle of both (i.e. at $45^\circ$), then somehow *more* light is let through than if the middle filter weren't there at all.^[For the more visually inclined, there is a [video on YouTube by minutephysics](https://www.youtube.com/watch?v=zcqZHYo7ONs) about this experiment, or you can play with a virtual version on the [Quantum Flytrap Virtual Lab](https://lab.quantumflytrap.com/lab/bell-inequality?mode=laser).]

This is intrinsically linked to **Bell's theorem**, which proves the technical sounding statement that "any local real hidden variable theory must satisfy certain statistical properties", which is _not_ satisfied in reality, as many quantum mechanical experiments (such as the above) show!





## Hidden variables

The story of "hidden variables" dates back to 1935 and grew out of Einstein's worries about the completeness of quantum theory.
Consider, for example, a single qubit.
Recalling our previous discussion on compatible operators (Section \@ref(compatible-observables-and-uncertainty)), we know that no quantum state of a qubit can be a simultaneous eigenstate of two *non-commuting* operators, such as $\sigma_x$ and $\sigma_z$.
Physically, this means that if the qubit has a definite value of $\sigma_x$ then its value of $\sigma_z$ must be indeterminate, and vice versa.
If we take quantum theory to be a complete description of the world, then we must accept that it is impossible for both $\sigma_x$ and $\sigma_z$ to have definite values for the same qubit at the same time.^[Here it's important that we're really talking about so-called **local** hidden variable theories. We discuss the technical details in \@ref(hidden-loopholes).]
Einstein felt very uncomfortable about all this: he argued that quantum theory is incomplete, and that observables $\sigma_x$ and $\sigma_z$ may both have simultaneous definite values, although we only have knowledge of one of them at a time.
This is the hypothesis of **hidden variables**.

In this view, the indeterminacy found in quantum theory is merely due to our ignorance of these "hidden variables" that are present in nature but not accounted for in the theory.
Einstein came up with a number of pretty good arguments for the existence of "hidden variables", perhaps the most compelling of which was described in his 1935 paper (known as "the EPR paper"), co-authored with his younger colleagues, Boris Podolsky and Nathan Rosen.
It stood for almost three decades as the most significant challenge to the completeness of quantum theory.
Then, in 1964, John Bell showed that the (local) hidden variable hypothesis can be tested and refuted.

::: {.idea latex=""}
Any theory can make predictions, but just because the predictions turn out to be correct, this does not make the theory true --- there may be other, maybe equivalent, explanations.
The key to the scientific method is **falsifiability**: make one prediction incorrectly, and you have proven your theory is not true.
:::

:::: {.technical title="No-go theorems" latex="{No-go theorems}"}
::: {.todo latex=""}
<!-- TO-DO: write this -->
<!-- TO-DO: address the points raised by Jedszej in his email -->
:::
<!-- there are other similar theorems, called **no-go theorems**:

- Bell's = no-go for *local* hidden variables
- Kochen--Specker = no-go for *non-contextual* hidden variables
- PBR = no-go for *preparation independent* hidden variables. -->
::::





## Quantum correlations {#quantum-correlations}

Consider two entangled qubits in the **singlet**^[We say that a system is **singlet** if all the qubits involved are entangled. For example, the Bell states (Section \@ref(bell-states)) are all singlet states. This is related to the notion of [**singlet states**](https://en.wikipedia.org/wiki/Singlet_state) in quantum mechanics, which are those with zero net angular momentum.] state
$$
  \ket{\psi}
  = \frac{1}{\sqrt{2}} \left( \ket{01}-\ket{10} \right)
$$
and note that the projector $\proj{\psi}$ can be written as
$$
  \proj{\psi}
  = \frac{1}{4} \left(
    \id\otimes\id - \sigma_x\otimes\sigma_x - \sigma_y\otimes\sigma_y - \sigma_z\otimes \sigma_z
  \right)
$$
where $\sigma_x$, $\sigma_y$, and $\sigma_z$ are our old friends the Pauli matrices.

Also recall that any single-qubit observable^[We say "observable" and "value" instead of "Hermitian operator" and "eigenvalue" because it's useful to be able to switch between speaking like a mathematician and like a physicist!] with values $\pm1$ can be represented by the operator
$$
  \vec{a}\cdot\vec\sigma
  = a_x\sigma_x + a_y\sigma_y + a_z\sigma_z,
$$
where $\vec{a}$ is a unit vector in the three-dimensional Euclidean space.

So if Alice and Bob both choose observables, then we can characterise their choice^[For example, if the two qubits are spin-half particles, they may measure the spin components along the directions $\vec{a}$ and $\vec{b}$.] by vectors $\vec{a}$ and $\vec{b}$, respectively.
If Alice measures the first qubit in our singlet state $\ket{\psi}$, and Bob the second, then the corresponding observable is described by the tensor product
$$
  A\otimes B
  = (\vec{a}\cdot\vec\sigma)\otimes(\vec{b}\cdot\vec\sigma).
$$
The eigenvalues of $A\otimes B$ are the products of eigenvalues of $A$ and $B$.
Thus $A\otimes B$ has two eigenvalues: $+1$, corresponding to the instances when Alice and Bob registered identical outcomes, i.e. $(+1,+1)$ or $(-1,-1)$; and $-1$, corresponding to the instances when Alice and Bob registered different outcomes, i.e. $(+1,-1)$ or $(-1,+1)$.

This means that the expected value^[Recall Section \@ref(observables): the expected value of an operator $E$ in the state $\ket{\phi}$ is equal to $\braket{\phi|E}{\phi}$.] of $A\otimes B$, in any state, has a simple interpretation:
$$
    \av{A\otimes B} = \Pr (\text{outcomes are the same}) - \Pr (\text{outcomes are different}).
$$
This expression can take any real value in the interval $[-1,1]$, where $-1$ means we have **perfect anti-correlations**, $0$ means **no correlations**, and $+1$ means **perfect correlations**.

We can evaluate the expectation value in the singlet state:
$$
\begin{aligned}
  \bra{\psi}A\otimes B\ket{\psi}
  & = \tr\Big[
      (\vec{a}\cdot\vec\sigma)\otimes(\vec{b}\cdot\vec\sigma) \proj{\psi}
    \Big]
\\& = -\frac{1}{4} \tr\Big[
      (\vec{a}\cdot\vec\sigma)\sigma_x \otimes(\vec{b}\cdot\vec\sigma)\sigma_x
      + (\vec{a}\cdot\vec\sigma)\sigma_y \otimes(\vec{b}\cdot\vec\sigma)\sigma_y
      + (\vec{a}\cdot\vec\sigma)\sigma_z \otimes(\vec{b}\cdot\vec\sigma)\sigma_z
    \Big]
\\& = -\frac{1}{4} \tr\Big[
      4(a_x b_x + a_y b_y + a_z b_z) \id\otimes\id
    \Big]
\\& = -\vec{a}\cdot\vec{b}
\end{aligned}
$$
where we have used the fact that $\tr(\vec{a}\cdot\vec\sigma)\sigma_k = 2a_k$ (for $k=x,y,z$).
So if Alice and Bob choose the same observable $\vec{a} = \vec{b}$, then the expected value $\av{A\otimes B}$ will be equal to $-1$, and their outcomes will *always* be opposite: whenever Alice registers $+1$ (resp. $-1$) Bob is bound to register $-1$ (resp. $+1$).





## The CHSH inequality {#chsh-inequality}

> An upper bound on **classical** correlations.

<div class="video" title="Device-independent tests and Bell inequalities" data-videoid="xFj9Mf9LGso"></div>

We will describe the most popular version of Bell's argument, introduced in 1969 by [John Clauser](https://en.wikipedia.org/wiki/John_Clauser), [Michael Horne](https://en.wikipedia.org/wiki/Michael_Horne_(physicist)), [Abner Shimony](https://en.wikipedia.org/wiki/Abner_Shimony), and [Richard Holt](https://en.wikipedia.org/wiki/Richard_Holt_(physicist)) (whence the name "CHSH").

Let us start by making this assumption that the results of any measurement on any individual system are predetermined --- any probabilities we may use to describe the system merely reflect our ignorance of these hidden variables.

Imagine the following scenario.
Alice and Bob, our two characters with a predilection for wacky experiments, are equipped with appropriate measuring devices and sent to two distant locations.
Assume that Alice and Bob each have a choice of *two* observables that they can measure, each with well defined^[The phrase "well defined" corresponds to our "hidden variable" assumption, i.e. that the observables *always* have *definite* values.] values $+1$ and $-1$.
Let's say that Alice can choose between observables $A_1$ and $A_2$, and Bob between $B_1$ and $B_2$.
Now, somewhere in between them there is a source that emits pairs of qubits that fly apart, one towards Alice and one towards Bob.
For each incoming qubit, Alice and Bob choose randomly, and independently from each other, which particular observable will be measured.
This means we can think of the observables as random variables $A_k,B_k$ (for $k=1,2$) that take values $\pm1$.
Using these, we can define a new random variable: the **CHSH quantity**
$$
  S = A_1(B_1 - B_2) + A_2(B_1 + B_2).
$$

By a case-by-case analysis of the four possible outcomes for the pair $(B_1,B_2)$, we see that one of the terms $B_1\pm B_2$ must be equal to zero and the other to $\pm 2$ (basically depending on if $B_1=B_2$ or not), and so (looking at the four possible outcomes for the pair $(A_1,A_2)$) we see that $S=\pm2$.
But the average value of $S$ must lie in between these two possible outcomes, i.e.
$$
  -2 \leq \av{S} \leq 2.
$$
That's it!
Such a simple and yet profound mathematical statement about correlations, which we refer simply to as the **CHSH inequality**.

::: {.idea latex=""}
There is absolutely *no quantum theory involved* in the CHSH inequality
$$
  -2 \leq \av{S} \leq 2
$$
because the CHSH inequality is not specific to quantum theory: it does not really matter what kind of physical process is behind the appearance of binary values of $A_1$, $A_2$, $B_1$, and $B_2$; it is merely a statement about correlations, and for all classical correlations we must have
$$
  |
    \av{A_1 B_1} - \av{A_1 B_2} + \av{A_2 B_1} + \av{A_2 B_2}
  | \leq 2
$$
(which is just another way of phrasing the CHSH inequality).
:::

There are essentially *two* (very important) assumptions here:

1. **Hidden variables.** Observables have definite values.
2. **Locality.** Alice's choice of measurements (choosing between $A_1$ and $A_2$) does not affect the outcomes of Bob's measurement, and vice versa.

We will not discuss the locality assumption right now in detail (see Section \@ref(hidden-loopholes)), but let us just give one brief comment.
In the hidden variable world a, statement such as "if Bob were to measure $B_1$ then he would register $+1$" must be either true or false (and not "undecidable" or some other such thing!) *prior to Bob's measurement*.
Without the locality hypothesis, such a statement is ambiguous, since the value of $B_1$ could depend on whether $A_1$ or $A_2$ will be chosen by Alice.
We do not want this since it implies *instantaneous communication* --- it means that, say, Alice by making a choice between $A_1$ and $A_2$ affects Bob's results: Bob can immediately "see" what Alice "does".

Now let's see how quantum theory *fundamentally disagrees* with the CHSH inequality.





## Bell's theorem from CHSH

<div class="video" title="Device-independent tests and Bell inequalities (continued)" data-videoid="P-hk1GnAE6k"></div>

Continuing this story of Alice and Bob with their observables and pairs of qubits, let us first rephrase things in the formalism of quantum mechanics that we've been building up.
The observables $A_1$, $A_2$, $B_1$, $B_2$ become $(2\times 2)$ Hermitian matrices, each with the two eigenvalues $\pm 1$, and $\av{S}$ becomes the expected value of the $(4\times 4)$ **CHSH matrix**
$$
  S = A_1\otimes(B_1-B_2) + A_2\otimes(B_1+B_2).
$$
We can now evaluate $\av{S}$ using quantum theory.

::: {.idea latex=""}
Actually performing these measurements described by $S$ on a pair of qubits is known as a **CHSH test**, or **Bell test**.
:::

If the two qubits are in the singlet state
$$
  \ket{\psi}
  = \frac{1}{\sqrt{2}} \left( \ket{01}-\ket{10} \right)
$$
then we have already seen (Section \@ref(quantum-correlations)) that
$$
  \av{A\otimes B} = -\vec{a}\cdot\vec{b}.
$$
So if we choose vectors $\vec{a}_1$, $\vec{a}_2$, $\vec{b}_1$, and $\vec{b}_2$ as shown in Figure \@ref(fig:choice-of-as-and-bs), then the corresponding matrices^[That is, $A_1=\vec{a}_1\cdot\vec{\sigma}$, and so on.] satisfy
$$
\begin{aligned}
  \av{A_1\otimes B_1}
  &= \av{A_2\otimes B_1}
  = \av{A_2\otimes B_2}
  = \frac{1}{\sqrt{2}}
\\\av{A_1\otimes B_2}
  &= -\frac{1}{\sqrt{2}}.
\end{aligned}
$$

(ref:choice-of-as-and-bs-caption) The relative angle between the two perpendicular pairs is $45^\circ$.

```{r choice-of-as-and-bs,engine='tikz',fig.width=1.5,fig.cap='(ref:choice-of-as-and-bs-caption)'}
\definecolor{primary}{RGB}{177,98,78}
\definecolor{secondary}{RGB}{91,132,177}
\begin{tikzpicture}
  \draw [->,primary,rotate=45] (0,0) -- (1,0) node [right] {$b_1$};
  \draw [->,primary,rotate=-45] (0,0) -- (1,0) node [below] {$b_2$};
  \draw [->,secondary,rotate=90] (0,0) -- (1,0) node [above] {$a_1$};
  \draw [->,secondary,rotate=00] (0,0) -- (1,0) node [right] {$a_2$};
\end{tikzpicture}
```

Plugging these values in, we get that
$$
  \av{A_1 B_1} - \av{A_1 B_2} + \av{A_2 B_1} + \av{A_2 B_2}
  = -2\sqrt{2},
$$
which obviously violates CHSH inequality: $-2\sqrt{2}$ is strictly less than $-2$!

But here is the really important part of this discussion: *this violation of the CHSH has been observed in a number of painstakingly careful experiments --- this is not just theoretical!*
The early efforts in these experiments were truly heroic, with many many layers of complexity; today, however, such experiments are routine.

::: {.idea latex=""}
**Bell's theorem.**
The behaviour of entangled quantum systems *cannot* be explained by local hidden variables.
In other words, outcomes in quantum mechanics really are random, and it's not simply our lack of knowledge about some background process.
:::

If we can enforce locality in an experimental setup (for example, by ensuring that Alice and Bob are sufficiently far apart so that there is not enough time between Alice making a measurement and Bob receiving his measurement result) then an experimental verification of the CHSH test *proves* to us that the system is behaving in an inherently non-classical and, importantly, *unpredictable* manner.
This means that this is a good test to see if our devices are performing as they are supposed to, and are untampered by any potential eavesdroppers.^[If an eavesdropper has observed our system to the extent that they can predict out outcomes, then that very predictability means that there is a hidden-variable description of the system, and the CHSH inequality is not violated.]
In other words, the CHSH test is key for securing quantum protocols, as we will explain in Section \@ref(quantum-randomness).





## Tsirelson's inequality

> An upper bound on **quantum** correlations.

One may ask if $|\av{S}|= 2\sqrt{2}$ is the *maximal* violation of the CHSH inequality, and the answer is "yes, it is": *quantum* correlations *always* satisfy the bound $|\av{S}|\leq2\sqrt{2}$.
This is because, no matter which state $\ket{\psi}$ we pick, the expected value $\av{S} = \braket{\psi}{S|\psi}$ cannot exceed the largest eigenvalue of $S$, and we can put an upper bound on the largest eigenvalues of $S$.
To start with, taking the largest eigenvalue (in absolute value) of a Hermitian matrix $M$, which we denote by $\|M\|$, gives a matrix norm, i.e. it has the following properties:
$$
\begin{aligned}
  \|M\otimes N\|
  & = \|M\| \|N\|
\\\|MN\|
  & \leq \|M\| \|N\|
\\\|M+N\|
  & \leq \|M\| + \|N\|
\end{aligned}
$$

Given that $\|A_k\|=\|B_k\|=1$ (for $k=1,2$), it is easy to use these properties to show that $\|S\|\leq4$, but this is a much weaker bound than we want.
However, one can show^[**Exercise.** Prove this!] that
$$
  S^2
  = 4(\id\otimes\id) + [A_1,A_2]\otimes[B_1,B_2].
$$

Now, the norms of the commutators $\|[A_1, A_2]\|$ and $\|[B_1, B_2]\|$ are bounded by^[**Exercise.** Prove this!] $2$, and $\|S^2\|=\|S\|^2$.
All together, this gives
$$
  \begin{aligned}
    \|S^2\|
    &\leq8
  \\\implies \|S\|
    &\leq2\sqrt{2}
  \\\implies |\av{S}|
    &\leq2\sqrt{2}
  \end{aligned}
$$
This result is known as the **Tsirelson inequality**.

::: {.idea latex=""}
In *classical* probability theory, the (absolute value of the) average value of the CHSH quantity
$$
  S = A_1(B_1 - B_2) + A_2(B_1 + B_2)
$$
is bounded by $2$, and this bound can be attained.

In *quantum* theory, the same value is bounded by $2\sqrt{2}$, and this bound can also be attained.
:::





## Quantum randomness {#quantum-randomness}

The experimental violations of the CHSH inequality have shown us that there are situations in which the measurement outcomes are truly unknown the instant before the measurement is made, and so the answer must be "chosen" randomly.
We can make use of this randomness is a number of different ways, the most obvious example of which being a random number generator.
Indeed, we have already met one suitable implementation:

```{r,engine='tikz',engine.opts=list(template="latex/tikz2pdf.tex"),fig.width=1.5}
\begin{quantikz}
  \lstick{$\ket{0}$} \qw
  & \gate{H}
  & \qw
\end{quantikz}
```

The state before measurement is $(\ket{0}+\ket{1})/\sqrt{2}$, so the two possible outcomes occur with equal probability.
This is a truly random number generator, not like the pseudo-random one that is used if you ask your computer for some random data.

This randomness generator works well as long as we know how it's been built, i.e. that it really is just a Hadamard gate, that the input qubit really has been prepared in the state $\ket{0}$, and that the measurement device is accurate and honest.
However, we don't all have a Hadamard gate and a supply of prepared qubits lying around at home, so it seems likely that at some point we might have to buy or borrow such a device from a third party.
But then how can we *know* that it really is doing what it promises, and not just supplying some pseudo-random numbers that might, for example, already be known to the manufacturer?
This would render the device useless for cryptographic purposes!
But not only do we want to know that this isn't the case, we would also like the average user to be able to verify this for themselves, without having to know about the internal details.
In other words, can we find a way of verifying the device via some analysis of just inputs and outputs?
This is the question of device independence.

::: {.idea latex=""}
A protocol is **device independent** if its security doesn't depend on trusting the devices on which it is implemented.
In other words, it has no reliance on trusting the third party who supply you with the devices.
:::

We can rule out one thing from the start, namely deterministic behaviour.
If we behave deterministically then we have no hope, since the third party can take this into account and potentially find a way to always fool us.
But there is another approach that we can try: rather than directly trying to verify the veracity of any given device, we can try to use it to turn a small amount of true randomness into a larger amount.
This is the idea of randomness expansion.

::: {.idea latex=""}
Starting from an initial seed of private randomness (completely unknown to any other party), **randomness expansion** is the process of extending this to a larger amount of randomness that remains completely private.
:::

Let's consider a different device: one that produces pairs of qubits in singlet states and gives one of its qubits to Alice and one to Bob.
If Alice then measures her qubit in the $X$ basis, and Bob measures his in the $Z$ basis (each keeping their outcomes private), they each obtain random bits that are independent of one another.
However, this is only really true if the device truly is giving them singlet states, and not predetermined unentangled states.
How can they test for this?

Using the idea of randomness expansion, let's assume that they start with some shared random private seed: some $m$-bit string that only they know.^[Note that we're pushing the problem somewhere else: how can they come up with this shared private seed in the first place? This is the problem of **key distribution**, and we'll return to this again later.]
They start by generating $n$ of these putative singlet states, and publicly decide on some value $0<p<1$.
With this, they randomly select $\lceil pn\rceil$ of the pairs to perform a CHSH test on.
Each test requires two random bits (to determine Alice and Bob's choice of measurement), so in total we will need the length $m$ of their shared random private seed be roughly
$$
  m
  \approx 2pn - pn\log_2 p - n(1-p)\log_2(1-p)
$$
where the $\log$ terms are approximately how many bits are required to randomly choose the subset of pairs to test.

Why does this help?
Well, if somebody has manipulated the device that produces the pairs, then they need to be sure that they haven't altered the pairs that Alice and Bob are testing on.
But they cannot know in advance which pairs that will be, and so they cannot risk manipulating anything.^[One can be much more quantitative about this by using Chernoff bounds for a simple strategy of "choose at random which pairs to manipulate", but a full proof of security is much more involved than we would like to be here.]

In the fraction $p$ of pairs where a CHSH test is performed, we get at least 1 bit of randomness out: Alice's measurement result is always chosen at random.
In fact, Bob's result will be partially correlated to Alice's, so we should be able to extract some more randomness from this as well, but we ignore this possibility for the sake of simplicity.
Thus in the end we create $(2-p)n$ bits of randomness.





## Loopholes in Bell tests {#hidden-loopholes}

::: {.todo latex=""}
<!-- TO-DO -->
:::






## *Remarks and exercises* {#remarks-and-exercises-EPR-bell}


### XOR games for randomness expansion {#xor-games-randomness-expansion}

The setup of the CHSH inequality that we have described can instead be imagined as a two-player all-or-nothing game between Alice and Bob, so let's study this type of game more generally.

- Alice and Bob each start with an integer **prompt** $a$ and $b$ (respectively), with $1\leq a,b\leq n$.
  This integer can come from anywhere: they could pick it themselves, or it could be given to them.
  Having seen this integer, a round of the game consists of them returning an **answer**, $x$ and $y$ (respectively), of length $m$ bits to an oracle.
  Both the prompt and the answer are kept secret, so that only the corresponding player knows them.

- They win if the **winning condition** computed by the oracle is $1$, and lose if it is $0$.
  The winning condition is given by
  $$
    \begin{cases}
      1 & \text{if }g_A(x,a,b) = g_B(y,a,b)
    \\0 & \text{if }g_A(x,a,b) \neq g_B(y,a,b)
    \end{cases}
  $$
  where $g_A$ and $g_B$ are deterministic one-bit-valued functions whose output values are equally likely to be $0$ or $1$ (i.e. they return the value $0$ for exactly half of the possible input triples).

- There exists a *quantum* strategy that always wins.
  It uses sets of $m$ measurements on a maximally entangled state, and the measurement operators for all possible settings either commute or anti-commute with one another.
  The measurements are specified by observables that are traceless (i.e. their trace is equal to $0$) and square to the identity.

- The best possible classical strategy fails $f$ of the time, for some fraction $f$.

A single player (say, Alice) can treat this as a single-player game and use it as the basis for a randomness expansion scheme.

::: {.todo latex=""}
<!-- TO-DO -->
:::


### XOR games for quantum key distribution

We can also use the setup from Exercise \@ref(xor-games-randomness-expansion) as the basis for a **quantum key distribution scheme**: a process that allows two people to produce a shared random string known only to them.

For the $i$-th round of the game, Alice and Bob randomly (and privately) select their prompts $a_i$ and $b_i$, which they then use to play one round of the game, giving answers $x_i$ and $y_i$.
They keep a note of all their prompts and answers, as well as the result of the winning condition for each round.
After many rounds, Alice and Bob publicly announce all their prompts $\{a_1,\ldots,a_n\}$ and $\{b_1,\ldots,b_n\}$.

This means that Alice, for example, now knows the following:^[Bob knows the same, but instead of Alice's answers $\{x_1,\ldots,x_n\}$ he knows his own $\{y_1,\ldots,y_n\}$.]

- both sets of prompts $\{a_1,\ldots,a_n\}$ and $\{b_1,\ldots,b_n\}$
- her answers $\{x_1,\ldots,x_n\}$
- the values of $i\in\{1,\ldots,n\}$ for which $g_A(x_i,a_i,b_i)=g_B(y_i,a_i,b_i)$, i.e. the numbers $i_1,\ldots,i_k$ of the rounds that they won.

This is enough information (given that we have run enough rounds) for Alice to determine $g_A$ and Bob to determine $g_B$.
The two of them can then each compute the $k$-bit string
$$
  g_A(x_{i_1},a_{i_1},b_{i_1})\ldots g_A(x_{i_k},a_{i_k},b_{i_k})
  = g_B(x_{i_1},a_{i_1},b_{i_1})\ldots g_B(x_{i_k},a_{i_k},b_{i_k})
$$
which gives their shared key.
Now they just need to deal with eavesdropping.

By publicly selecting a random selection of rounds and announcing the results of their putative $g_A$ and $g_B$ for these rounds, they can check whether or not their values are coherent with the result of the winning conditions determined by the oracle.^[For example, if they know that they lost round $i$, then it should be the case that $g_A(x_i,a_i,b_i)\neq g_B(y_i,a_i,b_i)$.]

- If everything is coherent with the results, then they can be sure (if they have played enough rounds and compared enough results) that there was no eavesdropping.
- If the fraction of tests that are coherent with the results is less than $f$, then they must assume that somebody has been eavesdropping, and they should cease communication.
- Anywhere in between these two cases, they can quantify how much an eavesdropper might know, and then run some method of privacy amplification to further exclude the eavesdropper (at the cost of shortening the key).


### Prescribed binary randomness

Find a quantum circuit^[*Hint: what is $\cos^2(\pi/4)$*?] that can act as a random source that outputs $0$ with probability $(2+\sqrt{2})/4$ and $1$ with probability $(2-\sqrt{2})/4$.


### Unbiasing bias

Suppose you have a source that produces random bits, but operates with a bias: it outputs $0$ with probability $p$ and $1$ with probability $1-p$, for some fixed $0<p<1$.

Find a method such that, given two outputs from this source, you successfully obtain a single unbiased random bit (i.e. as if the source had $p=1/2$) with probability $2p(1-p)$.
