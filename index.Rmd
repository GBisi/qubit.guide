--- 
published: false
title: "Lectures on Quantum Information Science"
author: "Artur Ekert and Tim Hosgood"
date: "Last updated: `r format(Sys.Date(), format='%B %d, %Y')`"
description: "An introductory series of lectures on quantum computing."
output:
  bookdown::gitbook:
    mathjax: null
    pandoc_args: ["--gladtex"]
    css: ["gitbook-custom.css", "katex.css"]
    includes:
      after_body: ["scripts.html"]
    split_by: chapter
    config:
      toc:
        before: |
          <li><h4>Table of contents</h4></li>
        collapse: section
      view: https://github.com/thosgood/quantum-info/blob/master/%s
      download: ["pdf"]
      fontsettings:
        theme: white
        family: serif
        size: 1
      sharing:
        facebook: no
        github: no
        twitter: no
        linkedin: no
        weibo: no
        instapaper: no
        vk: no
        all: []
      info: no
  bookdown::pdf_book:
    template: latex-template.tex
    latex_engine: xelatex
    keep_tex: yes
---

\providecommand{\xmapsto}[1]{\overset{#1}{\longmapsto}}
\providecommand{\bra}[1]{\langle#1|}
\providecommand{\ket}[1]{|#1\rangle}
\providecommand{\braket}[2]{\langle#1|#2\rangle}
\providecommand{\proj}[1]{|#1\rangle\langle#1|}
\providecommand{\av}[1]{\langle#1\rangle}
\providecommand{\tr}{\operatorname{tr}}
\providecommand{\id}{\mathbf{1}}
\providecommand{\diag}[2]{\begin{bmatrix}#1&0\\0&#2\end{bmatrix}}
\providecommand{\mqty}[1]{\begin{matrix}#1\end{matrix}}
\providecommand{\bmqty}[1]{\begin{bmatrix}#1\end{bmatrix}}
\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}

```{r echo=FALSE,warning=FALSE}
 library(knitr)
  opts_chunk$set(cache=TRUE,
                 echo=FALSE,
                 fig.pos='H',
                 fig.ext=if (knitr:::is_latex_output()) 'pdf' else 'png',
                 fig.align='center'
                )
```

# Introduction {-}

For the past however-many years, [Artur Ekert](https://www.arturekert.com/) has been teaching the masters course "Introduction to Quantum Information" at the University of Oxford.
During this time, many versions of accompanying lecture notes have come and gone, with constant improvements and changes being made.
The version that you will find on this website has been carefully edited by [Tim Hosgood](https://thosgood.com) into a cohesive "book", containing additional exercises and topics.
Thanks go to Zhenyu Cai for many helpful comments and corrections, and we also appreciate the work of Yihui Xie in developing the [Bookdown package](https://bookdown.org/yihui/bookdown/) with which this document was built.

For more information, see the [accompanying website](https://thosgood.com/quantum-info).

## Plan {-}

In this series of lectures you will learn how inherently quantum phenomena, such as quantum interference and quantum entanglement, can make information processing more efficient and more secure, even in the presence of noise.

The interdisciplinary nature of this topic, combined with the diverse backgrounds that different readers have, means that some may find some particular chapters easy, while others find them difficult.
The following will be assumed as prerequisites: elementary probability theory, complex numbers, vectors and matrices, tensor products, and Dirac bra-ket notation.
A basic knowledge of quantum mechanics (especially in the simple context of finite dimensional state spaces, e.g. state vectors, composite systems, unitary matrices, Born rule for quantum measurements) and some ideas from classical theoretical computer science (complexity theory) would be helpful, but is not at all essential.
Some of these things are covered at the end of this chapter.


## Topics {-}

- Fundamentals of quantum theory
  + addition of probability amplitudes
  + quantum interference
  + mathematical description of states and evolution of closed quantum systems (Hilbert space, unitary evolution)
  + measurements (projectors, Born rule)
  + Pauli matrices
- Distinguishability of quantum states
- The Bloch sphere
  + parametrisation
  + action of quantum gates on the Bloch vector
- The definition of quantum entanglement (the tensor product structure)
- The no-cloning theorem, and quantum teleportation
- Quantum gates
  + phase gate
  + Hadamard
  + controlled-$\texttt{NOT}$
  + SWAP
  + the Hadamard-phase-Hadamard network
  + phase "kick-back" induced by controlled-$U$
  + phase "kick-back" induced by quantum Boolean function evaluation
- Quantum algorithms
  + Deutsch
  + Bernstein-Vazirani
  + Simon
- Bell's theorem
  + Quantum correlations
  + CHSH inequality
- Density matrices
  + partial trace
  + statistical mixture of pure states
  + Born rule for density matrices
  + quantum entanglement in terms of density matrices
- Completely positive maps
  + Kraus operators
  + the Choi matrix
  + positive versus completely positive maps
  + partial-transpose
- The simple model of decoherence
- Quantum error correction of bit-flip and phase-flip errors





# Some mathematical preliminaries {-}

## Euclidean vectors

We assume that you are familiar with Euclidean vectors --- those arrow-like geometric objects which are used to represent physical quantities, such as velocities, or forces.
You know that any two velocities can be added to yield a third, and the multiplication of a "velocity vector" by a real number is another "velocity vector".
So a **linear combination** of vectors is another vector.
Mathematicians have simply taken these properties and defined vectors as _anything_ that we can add and multiply by numbers, as long as everything behaves in a nice enough way.
This is basically what an Italian mathematician Giuseppe Peano (1858--1932) did in a chapter of his 1888 book with an impressive title: _Calcolo geometrico secondo l'Ausdehnungslehre di H. Grassmann preceduto dalle operazioni della logica deduttiva_.


## Vector spaces

Following Peano, we define a **vector space** as a mathematical structure in which the notion of linear combination "makes sense".

More formally, a **complex vector space** is a set $V$ such that, given any two **vectors** $a$ and $b$ (that is, any two elements of $V$) and any two complex numbers $\alpha$ and $\beta$, we can form the linear combination^[As we said, there are certain "nice properties" that these things must satisfy. Addition of vectors must be commutative and associative, with an identity (the zero vector, which will always be written as $\mathbf{0}$ ) and an inverse for each $v$ (written as $-v$). Multiplication by complex numbers must obey the two distributive laws: $(\alpha+\beta)v = \alpha v+\beta v$ and $\alpha (v+w) = \alpha v+\alpha w$.] $\alpha a+\beta b$, which is also a vector in $V$.

A **subspace** of $V$ is any subset of $V$ which is closed under vector addition and multiplication by complex numbers.
Here we start using the Dirac bra-ket notation and write vectors in a somewhat fancy way as $\ket{\text{label}}$, where the "label" is anything that serves to specify what the vector is.
For example, $\ket{\uparrow}$ and $\ket{\downarrow}$ may refer to an electron with spin up or down along some prescribed direction and $\ket{0}$ and $\ket{1}$ may describe a quantum bit (a "qubit") holding either logical $0$ or $1$.
These are often called **ket** vectors, or simply **kets**.
(We will deal with "bras" in a moment).
A **basis** in $V$ is a collection of vectors $\ket{e_1},\ket{e_2},\ldots,\ket{e_n}$ such that every vector $\ket{v}$ in $V$ can be written (in _exactly_ one way) as a linear combination of the basis vectors; $\ket{v}=\sum_i v_i\ket{e_i}$.
The number of elements in a basis is called the **dimension** of $V$.
(Showing that this definition is independent of the basis that we choose is a "fun" linear algebra exercise).
The most common $n$-dimensional complex vector space is the space of ordered $n$-tuples of complex numbers, usually written as column vectors:
$$
  \begin{gathered}
    \ket{a}
    = \begin{bmatrix}a_1\\a_2\\\vdots\\a_n\end{bmatrix}
    \qquad
    \ket{b}
    = \begin{bmatrix}b_1\\b_2\\\vdots\\b_n\end{bmatrix}
  \\\alpha\ket{a}+\beta\ket{b}
    = \begin{bmatrix}\alpha a_1+\beta b_1\\\alpha a_2+\beta b_2\\\vdots\\\alpha a_n+\beta b_n\end{bmatrix}
  \end{gathered}
$$

In fact, this is the space we will use most of the time.
Throughout the course we will deal only with vector spaces of _finite_ dimensions.
This is sufficient for all our purposes and we will avoid many mathematical subtleties associated with infinite dimensional spaces, for which we would need to tools of **functional analysis**.


## Bras and kets

An **inner product** on a vector space $V$ (over the complex numbers) is a function that assigns to each pair of vectors $\ket{u},\ket{v}\in V$ a complex number $\braket{u}{v}$, and satisfies the following conditions:

- $\braket{u}{v}=\braket{v}{u}^\star$;
- $\braket{v}{v}\geq 0$ for all $\ket{v}$;
- $\braket{v}{v}= 0$ if and only if $\ket{v}=0$.

The inner product must also be _linear_ in the second argument but _antilinear_ in the first argument:
$$
  \braket{c_1u_1+c_2u_2}{v} = c_1^\star\braket{u_1}{v}+c_2^\star\braket{u_2}{v}
$$
for any complex constants $c_1$ and $c_2$.

With any physical system we associate a complex vector space with an inner product, known as a **Hilbert space**^[The term "Hilbert space" used to be reserved for an infinite-dimensional inner product space that is **complete**, i.e. such that every Cauchy sequence in the space converges to an element in the space. Nowadays, as in these notes, the term includes finite-dimensional spaces, which automatically satisfy the condition of completeness.] $\mathcal{H}$.
The inner product between vectors $\ket{u}$ and $\ket{v}$ in ${\mathcal{H}}$ is written as
$$
  \braket{u}{v}.
$$

For example, for column vectors $\ket{u}$ and $\ket{v}$ in $\mathbb{C}^n$ written as
$$
  \ket{u}
  = \begin{bmatrix}u_1\\u_2\\\vdots\\u_n\end{bmatrix}
  \qquad
  \ket{v}
  = \begin{bmatrix}v_1\\v_2\\\vdots\\v_n\end{bmatrix}
$$
their inner product is defined as
$$
  \braket{u}{v}
  = u_1^\star v_1 + u_2^\star v_2+\ldots + u_n^\star v_n.
$$
Following Dirac we may split the inner product into two ingredients
$$
  \braket{u}{v}
  \longrightarrow \bra{u}\,\ket{v}.
$$
Here $\ket{v}$ is a ket vector, and $\bra{u}$ is called a **bra** vector, or a **bra**, and can be represented by a row vector:
$$
  \bra{u}
  = \begin{bmatrix}u_1^\star,&u_2^\star,&\ldots,&u_n^\star\end{bmatrix}.
$$
The inner product can now be viewed as the result of the matrix multiplication:
$$
  \begin{aligned}
    \braket{u}{v}
    &= \begin{bmatrix}u_1^\star,&u_2^\star,&\ldots,&u_n^\star\end{bmatrix}
    \cdot \begin{bmatrix}v_1\\v_2\\\vdots\\v_n\end{bmatrix}
  \\&= u_1^\star v_1 + u_2^\star v_2 + \ldots + u_n^\star v_n.
  \end{aligned}
$$

Bras are vectors: you can add them, and multiply them by scalars (which, here, are complex numbers), but they are vectors in the space ${\mathcal{H}}^\star$ which is **dual** to $\mathcal{H}$.
Elements of ${\mathcal{H}}^\star$ are **linear functionals**, that is, linear maps from $\mathcal{H}$ to $\mathbb{C}$.
A linear functional $\bra{u}$ acting on a vector $\ket{v}$ in $\mathcal{H}$ gives a complex number $\braket{u}{v}$.

::: {.idea data-latex=""}
All Hilbert spaces of the same dimension are isomorphic, so the differences between quantum systems cannot be really understood without additional structure. This structure is provided by a specific algebra of operators acting on $\mathcal{H}$.
:::


## Daggers

Although $\mathcal{H}$ and $\mathcal{H}^\star$ are not identical spaces -- the former is inhabited by kets, and the latter by bras -- they are closely related.
There is a bijective map from one to the other, $\ket{v}\leftrightarrow \bra{v}$, denoted by a **dagger**:^["Is this a $\dagger$ which I see before me..."]
$$
  \begin{aligned}
    \bra{v}
    &= (\ket{v})^\dagger
  \\\ket{v}
    &= (\bra{v})^\dagger.
  \end{aligned}
$$
We usually omit the parentheses when it is obvious what the dagger operation applies to.

The dagger operation, also known as **Hermitian conjugation**, is _antilinear_: 
$$
  \begin{aligned}
    (c_1\ket{v_1}+c_2\ket{v_2})^\dagger
    &= c_1^\star\bra{v_1} + c_2^\star\bra{v_2}
  \\(c_1\bra{v_1}+c_2\bra{v_2})^\dagger
    &= c_1^\star\ket{v_1} + c_2^\star\ket{v_2}.
  \end{aligned}
$$
Also, when applied twice, the dagger operation is the identity map.
In the matrix representation,^[Recall that the conjugate transpose, or the Hermitian conjugate, of an $(n\times m)$ matrix $A$ is an $(m\times n)$ matrix $A^\dagger$, obtained by interchanging the rows and columns of $A$ and taking complex conjugates of each entry in $A$, i.e. $A^\dagger_{ij}=A^\star_{ji}$. In mathematics texts it is often denoted by ${}^\star$ rather than ${}^\dagger$.]
$$
  \ket{v} = \begin{bmatrix}v_1\\v_2\\\vdots\\v_n\end{bmatrix}
  \overset{\dagger}{\longleftrightarrow}
  \bra{v} = \begin{bmatrix}v_1^\star,&v_2^\star,&\ldots,&v_n^\star\end{bmatrix}.
$$


## Geometry

The inner product brings geometry: the **length**, or **norm**, of $\ket{v}$ is given by $\|v\|=\sqrt{\braket{v}{v}}$, and we say that $\ket{u}$ and $\ket{v}$ are **orthogonal** if $\braket{u}{v}=0$.
Any maximal set of pairwise orthogonal vectors of unit length^[That is, consider sets of vectors $\ket{e_i}$ such that $\braket{e_i}{e_j}=\delta_{ij}$ (where the **Kronecker delta** $\delta_{ij}$ is $0$ if $i\neq j$, and $1$ if $i=j$.), and then pick any of the largest such sets (which must exist, since we assume our vector spaces to be finite dimensional).] forms an orthonormal basis, and so any vector can be expressed as a linear combination of the basis vectors:
$$
  \begin{gathered}
    \ket{v}
    =\sum_i v_i\ket{e_i}
  \\\text{where $v_i=\braket{e_i}{v}$}.
  \end{gathered}
$$
Then the bras $\bra{e_i}$ form the **dual basis**
$$
  \begin{gathered}
    \bra{v}
    =\sum_i v_i^\star\bra{e_i}
  \\\text{where $v_i^\star=\braket{v}{e_i}$}.
  \end{gathered}
$$

To make the notation a bit less cumbersome, we will sometimes label the basis kets as $\ket{i}$ rather than $\ket{e_i}$, and write
$$
  \begin{aligned}
    \ket{v}
    &= \sum_i \ket{i}\braket{i}{v}
  \\\bra{v}
    &= \sum_j \braket{v}{i}\bra{i}.
  \end{aligned}
$$
But _do not confuse $\ket{0}$ with the zero vector_!
We _never_ write the zero vector as $\ket{0}$, but only ever as $0$, without any bra or ket decorations (so e.g. $\ket{v}+0=\ket{v}$).

::: {.idea data-latex=""}
With any _isolated_ quantum system, which can be prepared in $n$ _perfectly distinguishable_ states, we can associate a Hilbert space $\mathcal{H}$ of dimension $n$ such that each vector $\ket{v}\in\mathcal{H}$ of unit length (i.e. $\braket{v}{v} =1$) represents a quantum state of the system.

The overall phase of the vector has no physical significance: $\ket{v}$ and $e^{i\varphi}\ket{v}$ (for any real $\varphi$) both describe the same state.

The inner product $\braket{u}{v}$ is the _probability amplitude_ that a quantum system prepared in state $\ket{v}$ will be found in state $\ket{u}$ upon measurement.

States corresponding to orthogonal vectors (i.e. $\braket{u}{v}=0$) are _perfectly distinguishable_, since, if we prepare the system in state $\ket{v}$, then it will never be found in state $\ket{u}$, and vice versa.
In particular, states forming orthonormal bases are always perfectly distinguishable from each other.
Choosing such states, as we shall see in a moment, is equivalent to choosing a particular quantum measurement.
:::


## Operators

A **linear map** between two vector spaces $\mathcal{H}$ and $\mathcal{K}$ is a function $A\colon\mathcal{H}\to\mathcal{K}$ that respects linear combinations:
$$
  A(c_1\ket{v_1}+c_2\ket{v_2})=c_1 A\ket{v_1}+c_2 A\ket{v_2}
$$
for any vectors $\ket{v_1},\ket{v_2}$ and any complex numbers $c_1,c_2$.
We will focus mostly on **endomorphisms**, that is, maps from $\mathcal{H}$ to $\mathcal{H}$, and we will call them **operators**.
The symbol $\id$ is reserved for the identity operator that maps every element of $\mathcal{H}$ to itself (i.e. $\id\ket{v}=\ket{v}$ for all $\ket{v}\in\mathcal{H}$).
The product $AB$ of two operators $A$ and $B$ is the operator obtained by first applying $B$ to some ket $\ket{v}$ and then $A$ to the ket which results from applying $B$:
$$
  (AB)\ket{v} = A(B\ket{v}).
$$
The order _does_ matter: in general, $AB\neq BA$.
In the exceptional case in which $AB=BA$, one says that these two operators **commute**.
The inverse of $A$, written as $A^{-1}$, is the operator that satisfies $AA^{-1}=\id=A^{-1}A$.
For finite-dimensional spaces, one only needs to check _one_ of these two conditions, since any one of the two implies the other, whereas, on an infinite-dimensional space, _both_ must be checked.
Finally, given a particular basis, an operator $A$ is uniquely determined by the entries of its matrix, defined by $A_{ij}=\bra{i}A\ket{j}$.
The **adjoint**, or **Hermitian conjugate**, of $A$, denoted by $A^\dagger$, is defined by the relation
$$
  \begin{gathered}
    \bra{i}A^\dagger\ket{j}
    = \bra{j}A\ket{i}^\star
  \\\text{for all $\ket{i},\ket{j}\in\mathcal{H}$}.
  \end{gathered}
$$

An operator $A$ is said to be

- **normal** if $AA^\dagger = A^\dagger A$,
- **unitary** if $AA^\dagger = A^\dagger A = \id$,
- **Hermitian** (or **self-adjoint**) if $A^\dagger = A$.

Any physically admissible evolution of an isolated quantum system is represented by a unitary operator.
Note that unitary operators preserve the inner product: given a unitary operator $U$ and two kets $\ket{a}$ and $\ket{b}$, and defining $\ket{a'}=U\ket{a}$ and $\ket{b'}=U\ket{b}$, we have that
$$
  \begin{gathered}
    \bra{a'}=\bra{a}U^\dagger
  \\\bra{b'}=\bra{b}U^\dagger
  \\\braket{a'}{b'}=\bra{a}U^\dagger U\ket{b}=\bra{a}\id\ket{b}=\braket{a}{b}.
  \end{gathered}
$$
Preserving the inner product implies preserving the norm induced by this product, i.e. unit state vectors are mapped to unit state vectors, i.e. _unitary operations are the isometries of the Euclidean norm_.


## Outer products

Apart from the inner product $\braket{u}{v}$, which is a complex number, we can also form the **outer product** $\ket{u}\bra{v}$, which is a linear map (operator) on $\mathcal{H}$ (or on $\mathcal{H}^\star$, depending how you look at it).
This is what physicists like (and what mathematicians dislike!) about Dirac notation: a certain degree of healthy ambiguity.

- The result of $\ket{u}\bra{v}$ acting on a ket $\ket{x}$ is $\ket{u}\braket{v}{x}$, i.e. the vector $\ket{u}$ multiplied by the complex number $\braket{v}{x}$.
- Similarly, the result of $\ket{u}\bra{v}$ acting on a bra $\bra{y}$ is $\braket{y}{u}\bra{v}$, i.e. the functional $\bra{v}$ multiplied by the complex number $\braket{y}{u}$.

The product of two maps, $A=\ket{a}\bra{b}$ followed by $B=\ket{c}\bra{d}$, is a linear map $BA$, which can be written in Dirac notation as
$$
  BA = \ket{c}\braket{d}{a}\bra{b} = \braket{d}{a}\ket{c}\bra{b}
$$
i.e. the inner product (complex number) $\braket{d}{a}$ times the outer product (linear map) $\ket{c}\bra{b}$.

Any operator on $\mathcal{H}$ can be expressed as a sum of outer products. Given an orthonormal basis $\{\ket{e_i}\}$, any operator which maps the basis vectors $\ket{e_i}$ to vectors $\ket{f_i}$ can be written as $\sum_i\ket{f_i}\bra{e_i}$, where the sum is over all the vectors in the orthonormal basis.
If the vectors $\{\ket{f_i}\}$ also form an orthonormal basis then the operator simply "rotates" one orthonormal basis into another.
These are unitary operators which preserve the inner product.
In particular, if each $\ket{e_i}$ is mapped to $\ket{e_i}$, then we obtain the identity operator:
$$
  \sum_i\ket{e_i}\bra{e_i}=\id.
$$ 
This relation holds for _any_ orthonormal basis, and it is one of the most ubiquitous and useful formulas in quantum theory.
For example, for any vector $\ket{v}$ and for any orthonormal basis $\{\ket{e_i}\}$, we have
$$
  \begin{aligned}
    \ket{v}
    &= \id\ket{v}
  \\&= \sum_i \ket{e_i}\bra{e_i}\;\ket{v}
  \\&= \sum_i \ket{e_i}\;\braket{e_i}{v}
  \\&= \sum_i v_i\ket{e_i},
  \end{aligned}
$$
where $v_i=\braket{e_i}{v}$ are the components of $\ket{v}$.
Finally, note that the adjoint of $\ket{a}\bra{b}$ is $\ket{b}\bra{a}$.


## The trace

The **trace** is an operation which turns outer products into inner products,
$$
  \tr\colon \ket{b}\bra{a} \longmapsto \braket{a}{b}.
$$
We have just seen that any linear operator can be written as a sum of outer products, and so we can extend the definition of trace (by linearity) to any operator.
Alternatively, for any square matrix $A$, the trace of $A$ is defined to be the sum of its diagonal elements:
$$
  \tr A = \sum_k \bra{e_k}A\ket{e_k} = \sum_k A_{kk}.
$$
You can show, using this definition or otherwise, that the trace is cyclic (i.e. $\tr(AB) = \tr(BA)$) and linear (i.e. $\tr(\alpha A+\beta B) = \alpha\tr(A)+\beta\tr(B)$, where $A$ and $B$ are square matrices and $\alpha$ and $\beta$ complex numbers).
Moreover,
$$
  \begin{aligned}
    \tr\ket{b}\bra{a}
    &= \sum_k \braket{e_k}{b}\braket{a}{e_k}
  \\&= \sum_k \braket{a}{e_k}\braket{e_k}{b}
  \\&= \braket{a}{\id}\ket{b}
  \\&= \braket{a}{b}.
  \end{aligned}
$$
Here, the second term can be viewed both as the sum of the diagonal elements of $\ket{b}\bra{a}$ in the $\ket{e_k}$ basis, and as the sum of the products of two complex numbers $\braket{e_k}{b}$ and $\braket{a}{e_k}$.
We have used the decomposition of the identity, $\sum_k\ket{e_k}\bra{e_k}=\id$.
Given that we can decompose the identity by choosing any orthonormal basis, it is clear that the trace does _not_ depend on the choice of the basis.


## Some useful identities

- $\ket{a}^\dagger = \bra{a}$
- $\bra{a}^\dagger = \ket{a}$
- $(\alpha\ket{a}+\beta\ket{b})^\dagger = \alpha^\star\bra{a}+\beta^\star\bra{b}$
- $(\ket{a}\bra{b})^\dagger = \ket{b}\bra{a}$
- $(AB)^\dagger=B^\dagger A^\dagger$
- $(\alpha A+\beta B)^\dagger=\alpha^\star A^\dagger+\beta^\star B^\dagger$
- $(A^\dagger)^\dagger=A$
- $\tr(\alpha A+ \beta B) = \alpha \tr(A)+\beta\tr(B)$
- $\tr\ket{a}\bra{b} = \braket{b}{a}$
- $\tr(ABC) = \tr(CAB) = \tr(BCA)$
